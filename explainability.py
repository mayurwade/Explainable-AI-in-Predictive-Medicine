{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aa23a56-968a-4ae7-9241-e3c4bb052be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè• GALLSTONE PREDICTION MODEL - EXPLAINABILITY ANALYSIS\n",
      "============================================================\n",
      "=== LOADING DATA FILES ===\n",
      "üì¶ Loading model: best_model.pkl\n",
      "‚úÖ Model loaded: LGBMClassifier\n",
      "üì¶ Loading training data: X_train.pkl\n",
      "‚úÖ X_train loaded: (255, 38)\n",
      "üì¶ Loading test data: X_test.pkl\n",
      "‚úÖ X_test loaded: (64, 38)\n",
      "üì¶ Loading test labels: y_test.pkl\n",
      "‚úÖ y_test loaded: (64,)\n",
      "‚úÖ Explainer initialized:\n",
      "   Model: LGBMClassifier\n",
      "   Features: 38\n",
      "   Training samples: 255\n",
      "   Test samples: 64\n",
      "\n",
      "=== COMPARING FEATURE IMPORTANCE METHODS ===\n",
      "\n",
      "=== CALCULATING FEATURE IMPORTANCE ===\n",
      "üìä Using model's built-in feature importance...\n",
      "‚úÖ Model importance calculated\n",
      "üìä Calculating permutation importance...\n",
      "‚úÖ Permutation importance calculated\n",
      "‚úÖ Feature importance comparison saved as 'feature_importance_comparison.png'\n",
      "\n",
      "=== SETTING UP SHAP EXPLAINER ===\n",
      "üå≥ Using SHAP TreeExplainer for LightGBM...\n",
      "üìä Calculating SHAP values for 50 samples...\n",
      "‚úÖ SHAP values calculated: (50, 38)\n",
      "\n",
      "=== GENERATING SHAP PLOTS ===\n",
      "üìä Creating feature importance plot...\n",
      "‚úÖ Feature importance plot saved\n",
      "üìä Creating summary plot...\n",
      "‚úÖ Summary plot saved\n",
      "üìä Creating waterfall plot...\n",
      "‚úÖ Waterfall plot saved\n",
      "üéâ All SHAP plots saved to: shap_plots/\n",
      "\n",
      "=== GENERATING CLINICAL REPORT ===\n",
      "\n",
      "=== ANALYZING TOP 15 FEATURES ===\n",
      "üìä Top 15 Most Important Features:\n",
      "================================================================================\n",
      "‚¨áÔ∏è C-Reactive Protein (CRP)       | Impact:  -0.3038 | Decreases Risk\n",
      "‚¨ÜÔ∏è Vitamin D                      | Impact:   0.0310 | Increases Risk\n",
      "‚¨áÔ∏è Obesity (%)                    | Impact:  -0.1121 | Decreases Risk\n",
      "‚¨ÜÔ∏è Bone Mass (BM)                 | Impact:   0.0499 | Increases Risk\n",
      "‚¨áÔ∏è Extracellular Fluid/Total Body Water (ECF/TBW) | Impact:  -0.0681 | Decreases Risk\n",
      "‚¨ÜÔ∏è Body Protein Content (Protein) (%) | Impact:   0.1172 | Increases Risk\n",
      "‚¨ÜÔ∏è Visceral Muscle Area (VMA) (Kg) | Impact:   0.0960 | Increases Risk\n",
      "‚¨ÜÔ∏è Hemoglobin (HGB)               | Impact:   0.0395 | Increases Risk\n",
      "‚¨ÜÔ∏è Aspartat Aminotransferaz (AST) | Impact:   0.0046 | Increases Risk\n",
      "‚¨ÜÔ∏è Visceral Fat Area (VFA)        | Impact:   0.0118 | Increases Risk\n",
      "‚¨ÜÔ∏è Hepatic Fat Accumulation (HFA) | Impact:   0.0983 | Increases Risk\n",
      "‚¨áÔ∏è High Density Lipoprotein (HDL) | Impact:  -0.0290 | Decreases Risk\n",
      "‚¨áÔ∏è Creatinine                     | Impact:  -0.0838 | Decreases Risk\n",
      "‚¨áÔ∏è Alkaline Phosphatase (ALP)     | Impact:  -0.0285 | Decreases Risk\n",
      "‚¨ÜÔ∏è Total Fat Content (TFC)        | Impact:   0.0285 | Increases Risk\n",
      "\n",
      "============================================================\n",
      "CLINICAL INSIGHTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "üî¥ TOP RISK FACTORS (increase gallstone probability):\n",
      "   ‚Ä¢ Vitamin D                      (Impact: +0.0310)\n",
      "   ‚Ä¢ Bone Mass (BM)                 (Impact: +0.0499)\n",
      "   ‚Ä¢ Body Protein Content (Protein) (%) (Impact: +0.1172)\n",
      "   ‚Ä¢ Visceral Muscle Area (VMA) (Kg) (Impact: +0.0960)\n",
      "   ‚Ä¢ Hemoglobin (HGB)               (Impact: +0.0395)\n",
      "   ‚Ä¢ Aspartat Aminotransferaz (AST) (Impact: +0.0046)\n",
      "   ‚Ä¢ Visceral Fat Area (VFA)        (Impact: +0.0118)\n",
      "   ‚Ä¢ Hepatic Fat Accumulation (HFA) (Impact: +0.0983)\n",
      "   ‚Ä¢ Total Fat Content (TFC)        (Impact: +0.0285)\n",
      "   ‚Ä¢ Low Density Lipoprotein (LDL)  (Impact: +0.0089)\n",
      "\n",
      "üü¢ TOP PROTECTIVE FACTORS (decrease gallstone probability):\n",
      "   ‚Ä¢ C-Reactive Protein (CRP)       (Impact: -0.3038)\n",
      "   ‚Ä¢ Obesity (%)                    (Impact: -0.1121)\n",
      "   ‚Ä¢ Extracellular Fluid/Total Body Water (ECF/TBW) (Impact: -0.0681)\n",
      "   ‚Ä¢ High Density Lipoprotein (HDL) (Impact: -0.0290)\n",
      "   ‚Ä¢ Creatinine                     (Impact: -0.0838)\n",
      "\n",
      "üìà MODEL PERFORMANCE:\n",
      "   ‚Ä¢ Accuracy on test set: 0.828\n",
      "   ‚Ä¢ Positive predictions: 27/64 (42.2%)\n",
      "   ‚Ä¢ Average predicted probability: 0.468\n",
      "\n",
      "============================================================\n",
      "üéâ ANALYSIS COMPLETE!\n",
      "‚úÖ Check the generated files:\n",
      "   ‚Ä¢ shap_plots/ folder for SHAP visualizations\n",
      "   ‚Ä¢ feature_importance_comparison.png\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Working Gallstone Explainability Module - Fixed for Joblib Files\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib  # Using joblib instead of pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class GallstoneExplainability:\n",
    "    def __init__(self, model, X_train, X_test, y_test=None, feature_names=None):\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        # Handle feature names\n",
    "        if feature_names is None:\n",
    "            if hasattr(X_train, 'columns'):\n",
    "                self.feature_names = list(X_train.columns)\n",
    "            else:\n",
    "                self.feature_names = [f'Feature_{i}' for i in range(X_train.shape[1])]\n",
    "        else:\n",
    "            self.feature_names = feature_names\n",
    "            \n",
    "        self.shap_values = None\n",
    "        self.shap_explainer = None\n",
    "        self.lime_explainer = None\n",
    "        \n",
    "        print(f\"‚úÖ Explainer initialized:\")\n",
    "        print(f\"   Model: {type(self.model).__name__}\")\n",
    "        print(f\"   Features: {len(self.feature_names)}\")\n",
    "        print(f\"   Training samples: {len(self.X_train)}\")\n",
    "        print(f\"   Test samples: {len(self.X_test)}\")\n",
    "        \n",
    "    @classmethod\n",
    "    def load_from_files(cls, model_path=\"best_model.pkl\", X_train_path=\"X_train.pkl\", \n",
    "                       X_test_path=\"X_test.pkl\", y_test_path=\"y_test.pkl\"):\n",
    "        \"\"\"Load model and data from joblib files\"\"\"\n",
    "        try:\n",
    "            print(\"=== LOADING DATA FILES ===\")\n",
    "            \n",
    "            # Load model\n",
    "            print(f\"üì¶ Loading model: {model_path}\")\n",
    "            model = joblib.load(model_path)\n",
    "            print(f\"‚úÖ Model loaded: {type(model).__name__}\")\n",
    "            \n",
    "            # Load training data\n",
    "            print(f\"üì¶ Loading training data: {X_train_path}\")\n",
    "            X_train = joblib.load(X_train_path)\n",
    "            print(f\"‚úÖ X_train loaded: {X_train.shape}\")\n",
    "            \n",
    "            # Load test data\n",
    "            print(f\"üì¶ Loading test data: {X_test_path}\")\n",
    "            X_test = joblib.load(X_test_path)\n",
    "            print(f\"‚úÖ X_test loaded: {X_test.shape}\")\n",
    "            \n",
    "            # Load test labels (optional)\n",
    "            y_test = None\n",
    "            if os.path.exists(y_test_path):\n",
    "                try:\n",
    "                    print(f\"üì¶ Loading test labels: {y_test_path}\")\n",
    "                    y_test = joblib.load(y_test_path)\n",
    "                    print(f\"‚úÖ y_test loaded: {y_test.shape}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Could not load y_test: {str(e)}\")\n",
    "            \n",
    "            return cls(model, X_train, X_test, y_test)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading files: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def calculate_feature_importance(self):\n",
    "        \"\"\"Calculate feature importance using multiple methods\"\"\"\n",
    "        print(\"\\n=== CALCULATING FEATURE IMPORTANCE ===\")\n",
    "        importance_results = {}\n",
    "        \n",
    "        # 1. Model-specific feature importance (LightGBM has this)\n",
    "        if hasattr(self.model, 'feature_importances_'):\n",
    "            print(\"üìä Using model's built-in feature importance...\")\n",
    "            importance_results['model_importance'] = pd.DataFrame({\n",
    "                'feature': self.feature_names,\n",
    "                'importance': self.model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            print(\"‚úÖ Model importance calculated\")\n",
    "        \n",
    "        # 2. Permutation importance (if we have y_test)\n",
    "        if self.y_test is not None:\n",
    "            print(\"üìä Calculating permutation importance...\")\n",
    "            try:\n",
    "                perm_importance = permutation_importance(\n",
    "                    self.model,\n",
    "                    self.X_test,\n",
    "                    self.y_test,\n",
    "                    n_repeats=5,\n",
    "                    random_state=42,\n",
    "                    n_jobs=1  # Use single job to avoid multiprocessing issues\n",
    "                )\n",
    "                \n",
    "                importance_results['permutation_importance'] = pd.DataFrame({\n",
    "                    'feature': self.feature_names,\n",
    "                    'importance': perm_importance.importances_mean,\n",
    "                    'std': perm_importance.importances_std\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                print(\"‚úÖ Permutation importance calculated\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error calculating permutation importance: {str(e)}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Skipping permutation importance (no y_test provided)\")\n",
    "        \n",
    "        return importance_results\n",
    "    \n",
    "    def setup_shap_explainer(self, max_samples=50):\n",
    "        \"\"\"Initialize SHAP explainer\"\"\"\n",
    "        print(\"\\n=== SETTING UP SHAP EXPLAINER ===\")\n",
    "        \n",
    "        try:\n",
    "            # For LightGBM, use TreeExplainer\n",
    "            print(\"üå≥ Using SHAP TreeExplainer for LightGBM...\")\n",
    "            self.shap_explainer = shap.TreeExplainer(self.model)\n",
    "            \n",
    "            # Calculate SHAP values for a subset (to avoid memory issues)\n",
    "            n_samples = min(max_samples, len(self.X_test))\n",
    "            self.X_test_subset = self.X_test.iloc[:n_samples].copy()\n",
    "            \n",
    "            print(f\"üìä Calculating SHAP values for {n_samples} samples...\")\n",
    "            self.shap_values = self.shap_explainer.shap_values(self.X_test_subset)\n",
    "            \n",
    "            # Handle SHAP values format for binary classification\n",
    "            if isinstance(self.shap_values, list) and len(self.shap_values) == 2:\n",
    "                self.shap_values = self.shap_values[1]  # Use positive class\n",
    "                print(\"‚úÖ Using positive class SHAP values\")\n",
    "            \n",
    "            print(f\"‚úÖ SHAP values calculated: {self.shap_values.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error setting up SHAP explainer: {str(e)}\")\n",
    "            self.shap_explainer = None\n",
    "            self.shap_values = None\n",
    "    \n",
    "    def generate_shap_plots(self, save_path='shap_plots/'):\n",
    "        \"\"\"Generate SHAP visualizations\"\"\"\n",
    "        print(f\"\\n=== GENERATING SHAP PLOTS ===\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        if self.shap_values is None:\n",
    "            print(\"Setting up SHAP explainer first...\")\n",
    "            self.setup_shap_explainer()\n",
    "        \n",
    "        if self.shap_values is None:\n",
    "            print(\"‚ùå Cannot generate plots - SHAP values not available\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # 1. Feature importance bar plot\n",
    "            print(\"üìä Creating feature importance plot...\")\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(self.shap_values, self.X_test_subset, \n",
    "                            feature_names=self.feature_names, \n",
    "                            plot_type=\"bar\", show=False, max_display=15)\n",
    "            plt.title(\"SHAP Feature Importance (Top 15 Features)\", fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{save_path}/shap_importance_bar.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"‚úÖ Feature importance plot saved\")\n",
    "            \n",
    "            # 2. Summary plot (beeswarm)\n",
    "            print(\"üìä Creating summary plot...\")\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(self.shap_values, self.X_test_subset, \n",
    "                            feature_names=self.feature_names, show=False, max_display=15)\n",
    "            plt.title(\"SHAP Summary Plot (Feature Impact Distribution)\", fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{save_path}/shap_summary_beeswarm.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"‚úÖ Summary plot saved\")\n",
    "            \n",
    "            # 3. Waterfall plot for first prediction\n",
    "            print(\"üìä Creating waterfall plot...\")\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Create explanation object for waterfall plot\n",
    "            explanation = shap.Explanation(\n",
    "                values=self.shap_values[0], \n",
    "                base_values=self.shap_explainer.expected_value,\n",
    "                data=self.X_test_subset.iloc[0],\n",
    "                feature_names=self.feature_names\n",
    "            )\n",
    "            \n",
    "            shap.plots.waterfall(explanation, max_display=15, show=False)\n",
    "            plt.title(\"SHAP Waterfall Plot - Individual Prediction\", fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{save_path}/shap_waterfall.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"‚úÖ Waterfall plot saved\")\n",
    "            \n",
    "            print(f\"üéâ All SHAP plots saved to: {save_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating SHAP plots: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def analyze_top_features(self, top_n=10):\n",
    "        \"\"\"Analyze top contributing features\"\"\"\n",
    "        print(f\"\\n=== ANALYZING TOP {top_n} FEATURES ===\")\n",
    "        \n",
    "        if self.shap_values is None:\n",
    "            self.setup_shap_explainer()\n",
    "        \n",
    "        if self.shap_values is None:\n",
    "            print(\"‚ùå Cannot analyze features - SHAP values not available\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate mean absolute SHAP values\n",
    "        mean_abs_shap = np.abs(self.shap_values).mean(axis=0)\n",
    "        mean_shap = self.shap_values.mean(axis=0)\n",
    "        \n",
    "        # Create feature analysis dataframe\n",
    "        feature_analysis = pd.DataFrame({\n",
    "            'feature': self.feature_names,\n",
    "            'mean_abs_impact': mean_abs_shap,\n",
    "            'mean_impact': mean_shap,\n",
    "            'impact_direction': ['Increases Risk' if x > 0 else 'Decreases Risk' for x in mean_shap]\n",
    "        }).sort_values('mean_abs_impact', ascending=False)\n",
    "        \n",
    "        print(f\"üìä Top {top_n} Most Important Features:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        top_features = feature_analysis.head(top_n)\n",
    "        for idx, row in top_features.iterrows():\n",
    "            direction = \"‚¨ÜÔ∏è\" if row['mean_impact'] > 0 else \"‚¨áÔ∏è\"\n",
    "            print(f\"{direction} {row['feature']:<30} | Impact: {row['mean_impact']:>8.4f} | {row['impact_direction']}\")\n",
    "        \n",
    "        return feature_analysis\n",
    "    \n",
    "    def create_feature_importance_comparison(self):\n",
    "        \"\"\"Compare different feature importance methods\"\"\"\n",
    "        print(\"\\n=== COMPARING FEATURE IMPORTANCE METHODS ===\")\n",
    "        \n",
    "        importance_results = self.calculate_feature_importance()\n",
    "        \n",
    "        if not importance_results:\n",
    "            print(\"‚ùå No importance results available\")\n",
    "            return None\n",
    "        \n",
    "        # Create comparison plot\n",
    "        fig, axes = plt.subplots(1, len(importance_results), figsize=(6*len(importance_results), 8))\n",
    "        if len(importance_results) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, (method, df) in enumerate(importance_results.items()):\n",
    "            top_features = df.head(15)\n",
    "            \n",
    "            axes[idx].barh(range(len(top_features)), top_features['importance'])\n",
    "            axes[idx].set_yticks(range(len(top_features)))\n",
    "            axes[idx].set_yticklabels(top_features['feature'], fontsize=10)\n",
    "            axes[idx].set_xlabel('Importance')\n",
    "            axes[idx].set_title(f'{method.replace(\"_\", \" \").title()}')\n",
    "            axes[idx].invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"‚úÖ Feature importance comparison saved as 'feature_importance_comparison.png'\")\n",
    "        return importance_results\n",
    "    \n",
    "    def generate_clinical_report(self):\n",
    "        \"\"\"Generate a clinical summary report\"\"\"\n",
    "        print(\"\\n=== GENERATING CLINICAL REPORT ===\")\n",
    "        \n",
    "        # Analyze features\n",
    "        feature_analysis = self.analyze_top_features(15)\n",
    "        if feature_analysis is None:\n",
    "            return None\n",
    "        \n",
    "        # Separate risk and protective factors\n",
    "        risk_factors = feature_analysis[feature_analysis['mean_impact'] > 0].head(10)\n",
    "        protective_factors = feature_analysis[feature_analysis['mean_impact'] < 0].head(5)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CLINICAL INSIGHTS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nüî¥ TOP RISK FACTORS (increase gallstone probability):\")\n",
    "        for idx, row in risk_factors.iterrows():\n",
    "            print(f\"   ‚Ä¢ {row['feature']:<30} (Impact: +{row['mean_impact']:.4f})\")\n",
    "        \n",
    "        print(f\"\\nüü¢ TOP PROTECTIVE FACTORS (decrease gallstone probability):\")\n",
    "        for idx, row in protective_factors.iterrows():\n",
    "            print(f\"   ‚Ä¢ {row['feature']:<30} (Impact: {row['mean_impact']:.4f})\")\n",
    "        \n",
    "        print(f\"\\nüìà MODEL PERFORMANCE:\")\n",
    "        # Try to get some basic predictions\n",
    "        try:\n",
    "            predictions = self.model.predict(self.X_test)\n",
    "            pred_proba = self.model.predict_proba(self.X_test)\n",
    "            \n",
    "            if self.y_test is not None:\n",
    "                accuracy = (predictions == self.y_test).mean()\n",
    "                print(f\"   ‚Ä¢ Accuracy on test set: {accuracy:.3f}\")\n",
    "            \n",
    "            print(f\"   ‚Ä¢ Positive predictions: {predictions.sum()}/{len(predictions)} ({predictions.mean():.1%})\")\n",
    "            print(f\"   ‚Ä¢ Average predicted probability: {pred_proba[:, 1].mean():.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚Ä¢ Could not calculate performance metrics: {str(e)}\")\n",
    "        \n",
    "        return {\n",
    "            'feature_analysis': feature_analysis,\n",
    "            'risk_factors': risk_factors,\n",
    "            'protective_factors': protective_factors\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the complete explainability analysis\"\"\"\n",
    "    print(\"üè• GALLSTONE PREDICTION MODEL - EXPLAINABILITY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load the explainer\n",
    "    explainer = GallstoneExplainability.load_from_files()\n",
    "    \n",
    "    if explainer is None:\n",
    "        print(\"‚ùå Failed to load explainer\")\n",
    "        return\n",
    "    \n",
    "    # Run feature importance analysis\n",
    "    importance_results = explainer.create_feature_importance_comparison()\n",
    "    \n",
    "    # Run SHAP analysis\n",
    "    explainer.setup_shap_explainer()\n",
    "    \n",
    "    # Generate plots\n",
    "    explainer.generate_shap_plots()\n",
    "    \n",
    "    # Generate clinical report\n",
    "    clinical_report = explainer.generate_clinical_report()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ ANALYSIS COMPLETE!\")\n",
    "    print(\"‚úÖ Check the generated files:\")\n",
    "    print(\"   ‚Ä¢ shap_plots/ folder for SHAP visualizations\")\n",
    "    print(\"   ‚Ä¢ feature_importance_comparison.png\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f1959-f6c7-4fee-b804-22bbc01b2989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
