{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aa23a56-968a-4ae7-9241-e3c4bb052be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¥ GALLSTONE PREDICTION MODEL - EXPLAINABILITY ANALYSIS\n",
      "============================================================\n",
      "=== LOADING DATA FILES ===\n",
      "ğŸ“¦ Loading model: best_model.pkl\n",
      "âœ… Model loaded: LGBMClassifier\n",
      "ğŸ“¦ Loading training data: X_train.pkl\n",
      "âœ… X_train loaded: (255, 38)\n",
      "ğŸ“¦ Loading test data: X_test.pkl\n",
      "âœ… X_test loaded: (64, 38)\n",
      "ğŸ“¦ Loading test labels: y_test.pkl\n",
      "âœ… y_test loaded: (64,)\n",
      "âœ… Explainer initialized:\n",
      "   Model: LGBMClassifier\n",
      "   Features: 38\n",
      "   Training samples: 255\n",
      "   Test samples: 64\n",
      "\n",
      "=== COMPARING FEATURE IMPORTANCE METHODS ===\n",
      "\n",
      "=== CALCULATING FEATURE IMPORTANCE ===\n",
      "ğŸ“Š Using model's built-in feature importance...\n",
      "âœ… Model importance calculated\n",
      "ğŸ“Š Calculating permutation importance...\n",
      "âœ… Permutation importance calculated\n",
      "âœ… Feature importance comparison saved as 'feature_importance_comparison.png'\n",
      "\n",
      "=== SETTING UP SHAP EXPLAINER ===\n",
      "ğŸŒ³ Using SHAP TreeExplainer for LightGBM...\n",
      "ğŸ“Š Calculating SHAP values for 50 samples...\n",
      "âœ… SHAP values calculated: (50, 38)\n",
      "\n",
      "=== GENERATING SHAP PLOTS ===\n",
      "ğŸ“Š Creating feature importance plot...\n",
      "âœ… Feature importance plot saved\n",
      "ğŸ“Š Creating summary plot...\n",
      "âœ… Summary plot saved\n",
      "ğŸ“Š Creating waterfall plot...\n",
      "âœ… Waterfall plot saved\n",
      "ğŸ‰ All SHAP plots saved to: shap_plots/\n",
      "\n",
      "=== GENERATING CLINICAL REPORT ===\n",
      "\n",
      "=== ANALYZING TOP 15 FEATURES ===\n",
      "ğŸ“Š Top 15 Most Important Features:\n",
      "================================================================================\n",
      "â¬‡ï¸ C-Reactive Protein (CRP)       | Impact:  -0.3038 | Decreases Risk\n",
      "â¬†ï¸ Vitamin D                      | Impact:   0.0310 | Increases Risk\n",
      "â¬‡ï¸ Obesity (%)                    | Impact:  -0.1121 | Decreases Risk\n",
      "â¬†ï¸ Bone Mass (BM)                 | Impact:   0.0499 | Increases Risk\n",
      "â¬‡ï¸ Extracellular Fluid/Total Body Water (ECF/TBW) | Impact:  -0.0681 | Decreases Risk\n",
      "â¬†ï¸ Body Protein Content (Protein) (%) | Impact:   0.1172 | Increases Risk\n",
      "â¬†ï¸ Visceral Muscle Area (VMA) (Kg) | Impact:   0.0960 | Increases Risk\n",
      "â¬†ï¸ Hemoglobin (HGB)               | Impact:   0.0395 | Increases Risk\n",
      "â¬†ï¸ Aspartat Aminotransferaz (AST) | Impact:   0.0046 | Increases Risk\n",
      "â¬†ï¸ Visceral Fat Area (VFA)        | Impact:   0.0118 | Increases Risk\n",
      "â¬†ï¸ Hepatic Fat Accumulation (HFA) | Impact:   0.0983 | Increases Risk\n",
      "â¬‡ï¸ High Density Lipoprotein (HDL) | Impact:  -0.0290 | Decreases Risk\n",
      "â¬‡ï¸ Creatinine                     | Impact:  -0.0838 | Decreases Risk\n",
      "â¬‡ï¸ Alkaline Phosphatase (ALP)     | Impact:  -0.0285 | Decreases Risk\n",
      "â¬†ï¸ Total Fat Content (TFC)        | Impact:   0.0285 | Increases Risk\n",
      "\n",
      "============================================================\n",
      "CLINICAL INSIGHTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "ğŸ”´ TOP RISK FACTORS (increase gallstone probability):\n",
      "   â€¢ Vitamin D                      (Impact: +0.0310)\n",
      "   â€¢ Bone Mass (BM)                 (Impact: +0.0499)\n",
      "   â€¢ Body Protein Content (Protein) (%) (Impact: +0.1172)\n",
      "   â€¢ Visceral Muscle Area (VMA) (Kg) (Impact: +0.0960)\n",
      "   â€¢ Hemoglobin (HGB)               (Impact: +0.0395)\n",
      "   â€¢ Aspartat Aminotransferaz (AST) (Impact: +0.0046)\n",
      "   â€¢ Visceral Fat Area (VFA)        (Impact: +0.0118)\n",
      "   â€¢ Hepatic Fat Accumulation (HFA) (Impact: +0.0983)\n",
      "   â€¢ Total Fat Content (TFC)        (Impact: +0.0285)\n",
      "   â€¢ Low Density Lipoprotein (LDL)  (Impact: +0.0089)\n",
      "\n",
      "ğŸŸ¢ TOP PROTECTIVE FACTORS (decrease gallstone probability):\n",
      "   â€¢ C-Reactive Protein (CRP)       (Impact: -0.3038)\n",
      "   â€¢ Obesity (%)                    (Impact: -0.1121)\n",
      "   â€¢ Extracellular Fluid/Total Body Water (ECF/TBW) (Impact: -0.0681)\n",
      "   â€¢ High Density Lipoprotein (HDL) (Impact: -0.0290)\n",
      "   â€¢ Creatinine                     (Impact: -0.0838)\n",
      "\n",
      "ğŸ“ˆ MODEL PERFORMANCE:\n",
      "   â€¢ Accuracy on test set: 0.828\n",
      "   â€¢ Positive predictions: 27/64 (42.2%)\n",
      "   â€¢ Average predicted probability: 0.468\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ ANALYSIS COMPLETE!\n",
      "âœ… Check the generated files:\n",
      "   â€¢ shap_plots/ folder for SHAP visualizations\n",
      "   â€¢ feature_importance_comparison.png\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Working Gallstone Explainability Module - Fixed for Joblib Files\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib  # Using joblib instead of pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class GallstoneExplainability:\n",
    "    def __init__(self, model, X_train, X_test, y_test=None, feature_names=None):\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        # Handle feature names\n",
    "        if feature_names is None:\n",
    "            if hasattr(X_train, 'columns'):\n",
    "                self.feature_names = list(X_train.columns)\n",
    "            else:\n",
    "                self.feature_names = [f'Feature_{i}' for i in range(X_train.shape[1])]\n",
    "        else:\n",
    "            self.feature_names = feature_names\n",
    "            \n",
    "        self.shap_values = None\n",
    "        self.shap_explainer = None\n",
    "        self.lime_explainer = None\n",
    "        \n",
    "        print(f\"âœ… Explainer initialized:\")\n",
    "        print(f\"   Model: {type(self.model).__name__}\")\n",
    "        print(f\"   Features: {len(self.feature_names)}\")\n",
    "        print(f\"   Training samples: {len(self.X_train)}\")\n",
    "        print(f\"   Test samples: {len(self.X_test)}\")\n",
    "        \n",
    "    @classmethod\n",
    "    def load_from_files(cls, model_path=\"best_model.pkl\", X_train_path=\"X_train.pkl\", \n",
    "                       X_test_path=\"X_test.pkl\", y_test_path=\"y_test.pkl\"):\n",
    "        \"\"\"Load model and data from joblib files\"\"\"\n",
    "        try:\n",
    "            print(\"=== LOADING DATA FILES ===\")\n",
    "            \n",
    "            # Load model\n",
    "            print(f\"ğŸ“¦ Loading model: {model_path}\")\n",
    "            model = joblib.load(model_path)\n",
    "            print(f\"âœ… Model loaded: {type(model).__name__}\")\n",
    "            \n",
    "            # Load training data\n",
    "            print(f\"ğŸ“¦ Loading training data: {X_train_path}\")\n",
    "            X_train = joblib.load(X_train_path)\n",
    "            print(f\"âœ… X_train loaded: {X_train.shape}\")\n",
    "            \n",
    "            # Load test data\n",
    "            print(f\"ğŸ“¦ Loading test data: {X_test_path}\")\n",
    "            X_test = joblib.load(X_test_path)\n",
    "            print(f\"âœ… X_test loaded: {X_test.shape}\")\n",
    "            \n",
    "            # Load test labels (optional)\n",
    "            y_test = None\n",
    "            if os.path.exists(y_test_path):\n",
    "                try:\n",
    "                    print(f\"ğŸ“¦ Loading test labels: {y_test_path}\")\n",
    "                    y_test = joblib.load(y_test_path)\n",
    "                    print(f\"âœ… y_test loaded: {y_test.shape}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Could not load y_test: {str(e)}\")\n",
    "            \n",
    "            return cls(model, X_train, X_test, y_test)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading files: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def calculate_feature_importance(self):\n",
    "        \"\"\"Calculate feature importance using multiple methods\"\"\"\n",
    "        print(\"\\n=== CALCULATING FEATURE IMPORTANCE ===\")\n",
    "        importance_results = {}\n",
    "        \n",
    "        # 1. Model-specific feature importance (LightGBM has this)\n",
    "        if hasattr(self.model, 'feature_importances_'):\n",
    "            print(\"ğŸ“Š Using model's built-in feature importance...\")\n",
    "            importance_results['model_importance'] = pd.DataFrame({\n",
    "                'feature': self.feature_names,\n",
    "                'importance': self.model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            print(\"âœ… Model importance calculated\")\n",
    "        \n",
    "        # 2. Permutation importance (if we have y_test)\n",
    "        if self.y_test is not None:\n",
    "            print(\"ğŸ“Š Calculating permutation importance...\")\n",
    "            try:\n",
    "                perm_importance = permutation_importance(\n",
    "                    self.model,\n",
    "                    self.X_test,\n",
    "                    self.y_test,\n",
    "                    n_repeats=5,\n",
    "                    random_state=42,\n",
    "                    n_jobs=1  # Use single job to avoid multiprocessing issues\n",
    "                )\n",
    "                \n",
    "                importance_results['permutation_importance'] = pd.DataFrame({\n",
    "                    'feature': self.feature_names,\n",
    "                    'importance': perm_importance.importances_mean,\n",
    "                    'std': perm_importance.importances_std\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                print(\"âœ… Permutation importance calculated\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error calculating permutation importance: {str(e)}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Skipping permutation importance (no y_test provided)\")\n",
    "        \n",
    "        return importance_results\n",
    "    \n",
    "    def setup_shap_explainer(self, max_samples=50):\n",
    "        \"\"\"Initialize SHAP explainer\"\"\"\n",
    "        print(\"\\n=== SETTING UP SHAP EXPLAINER ===\")\n",
    "        \n",
    "        try:\n",
    "            # For LightGBM, use TreeExplainer\n",
    "            print(\"ğŸŒ³ Using SHAP TreeExplainer for LightGBM...\")\n",
    "            self.shap_explainer = shap.TreeExplainer(self.model)\n",
    "            \n",
    "            # Calculate SHAP values for a subset (to avoid memory issues)\n",
    "            n_samples = min(max_samples, len(self.X_test))\n",
    "            self.X_test_subset = self.X_test.iloc[:n_samples].copy()\n",
    "            \n",
    "            print(f\"ğŸ“Š Calculating SHAP values for {n_samples} samples...\")\n",
    "            self.shap_values = self.shap_explainer.shap_values(self.X_test_subset)\n",
    "            \n",
    "            # Handle SHAP values format for binary classification\n",
    "            if isinstance(self.shap_values, list) and len(self.shap_values) == 2:\n",
    "                self.shap_values = self.shap_values[1]  # Use positive class\n",
    "                print(\"âœ… Using positive class SHAP values\")\n",
    "            \n",
    "            print(f\"âœ… SHAP values calculated: {self.shap_values.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error setting up SHAP explainer: {str(e)}\")\n",
    "            self.shap_explainer = None\n",
    "            self.shap_values = None\n",
    "    \n",
    "    def generate_shap_plots(self, save_path='shap_plots/'):\n",
    "        \"\"\"Generate SHAP visualizations\"\"\"\n",
    "        print(f\"\\n=== GENERATING SHAP PLOTS ===\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        if self.shap_values is None:\n",
    "            print(\"Setting up SHAP explainer first...\")\n",
    "            self.setup_shap_explainer()\n",
    "        \n",
    "        if self.shap_values is None:\n",
    "            print(\"âŒ Cannot generate plots - SHAP values not available\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # 1. Feature importance bar plot\n",
    "            print(\"ğŸ“Š Creating feature importance plot...\")\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(self.shap_values, self.X_test_subset, \n",
    "                            feature_names=self.feature_names, \n",
    "                            plot_type=\"bar\", show=False, max_display=15)\n",
    "            plt.title(\"SHAP Feature Importance (Top 15 Features)\", fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{save_path}/shap_importance_bar.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"âœ… Feature importance plot saved\")\n",
    "            \n",
    "            # 2. Summary plot (beeswarm)\n",
    "            print(\"ğŸ“Š Creating summary plot...\")\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(self.shap_values, self.X_test_subset, \n",
    "                            feature_names=self.feature_names, show=False, max_display=15)\n",
    "            plt.title(\"SHAP Summary Plot (Feature Impact Distribution)\", fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{save_path}/shap_summary_beeswarm.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"âœ… Summary plot saved\")\n",
    "            \n",
    "            # 3. Waterfall plot for first prediction\n",
    "            print(\"ğŸ“Š Creating waterfall plot...\")\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Create explanation object for waterfall plot\n",
    "            explanation = shap.Explanation(\n",
    "                values=self.shap_values[0], \n",
    "                base_values=self.shap_explainer.expected_value,\n",
    "                data=self.X_test_subset.iloc[0],\n",
    "                feature_names=self.feature_names\n",
    "            )\n",
    "            \n",
    "            shap.plots.waterfall(explanation, max_display=15, show=False)\n",
    "            plt.title(\"SHAP Waterfall Plot - Individual Prediction\", fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{save_path}/shap_waterfall.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"âœ… Waterfall plot saved\")\n",
    "            \n",
    "            print(f\"ğŸ‰ All SHAP plots saved to: {save_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error generating SHAP plots: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def analyze_top_features(self, top_n=10):\n",
    "        \"\"\"Analyze top contributing features\"\"\"\n",
    "        print(f\"\\n=== ANALYZING TOP {top_n} FEATURES ===\")\n",
    "        \n",
    "        if self.shap_values is None:\n",
    "            self.setup_shap_explainer()\n",
    "        \n",
    "        if self.shap_values is None:\n",
    "            print(\"âŒ Cannot analyze features - SHAP values not available\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate mean absolute SHAP values\n",
    "        mean_abs_shap = np.abs(self.shap_values).mean(axis=0)\n",
    "        mean_shap = self.shap_values.mean(axis=0)\n",
    "        \n",
    "        # Create feature analysis dataframe\n",
    "        feature_analysis = pd.DataFrame({\n",
    "            'feature': self.feature_names,\n",
    "            'mean_abs_impact': mean_abs_shap,\n",
    "            'mean_impact': mean_shap,\n",
    "            'impact_direction': ['Increases Risk' if x > 0 else 'Decreases Risk' for x in mean_shap]\n",
    "        }).sort_values('mean_abs_impact', ascending=False)\n",
    "        \n",
    "        print(f\"ğŸ“Š Top {top_n} Most Important Features:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        top_features = feature_analysis.head(top_n)\n",
    "        for idx, row in top_features.iterrows():\n",
    "            direction = \"â¬†ï¸\" if row['mean_impact'] > 0 else \"â¬‡ï¸\"\n",
    "            print(f\"{direction} {row['feature']:<30} | Impact: {row['mean_impact']:>8.4f} | {row['impact_direction']}\")\n",
    "        \n",
    "        return feature_analysis\n",
    "    \n",
    "    def create_feature_importance_comparison(self):\n",
    "        \"\"\"Compare different feature importance methods\"\"\"\n",
    "        print(\"\\n=== COMPARING FEATURE IMPORTANCE METHODS ===\")\n",
    "        \n",
    "        importance_results = self.calculate_feature_importance()\n",
    "        \n",
    "        if not importance_results:\n",
    "            print(\"âŒ No importance results available\")\n",
    "            return None\n",
    "        \n",
    "        # Create comparison plot\n",
    "        fig, axes = plt.subplots(1, len(importance_results), figsize=(6*len(importance_results), 8))\n",
    "        if len(importance_results) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, (method, df) in enumerate(importance_results.items()):\n",
    "            top_features = df.head(15)\n",
    "            \n",
    "            axes[idx].barh(range(len(top_features)), top_features['importance'])\n",
    "            axes[idx].set_yticks(range(len(top_features)))\n",
    "            axes[idx].set_yticklabels(top_features['feature'], fontsize=10)\n",
    "            axes[idx].set_xlabel('Importance')\n",
    "            axes[idx].set_title(f'{method.replace(\"_\", \" \").title()}')\n",
    "            axes[idx].invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"âœ… Feature importance comparison saved as 'feature_importance_comparison.png'\")\n",
    "        return importance_results\n",
    "    \n",
    "    def generate_clinical_report(self):\n",
    "        \"\"\"Generate a clinical summary report\"\"\"\n",
    "        print(\"\\n=== GENERATING CLINICAL REPORT ===\")\n",
    "        \n",
    "        # Analyze features\n",
    "        feature_analysis = self.analyze_top_features(15)\n",
    "        if feature_analysis is None:\n",
    "            return None\n",
    "        \n",
    "        # Separate risk and protective factors\n",
    "        risk_factors = feature_analysis[feature_analysis['mean_impact'] > 0].head(10)\n",
    "        protective_factors = feature_analysis[feature_analysis['mean_impact'] < 0].head(5)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CLINICAL INSIGHTS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nğŸ”´ TOP RISK FACTORS (increase gallstone probability):\")\n",
    "        for idx, row in risk_factors.iterrows():\n",
    "            print(f\"   â€¢ {row['feature']:<30} (Impact: +{row['mean_impact']:.4f})\")\n",
    "        \n",
    "        print(f\"\\nğŸŸ¢ TOP PROTECTIVE FACTORS (decrease gallstone probability):\")\n",
    "        for idx, row in protective_factors.iterrows():\n",
    "            print(f\"   â€¢ {row['feature']:<30} (Impact: {row['mean_impact']:.4f})\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ MODEL PERFORMANCE:\")\n",
    "        # Try to get some basic predictions\n",
    "        try:\n",
    "            predictions = self.model.predict(self.X_test)\n",
    "            pred_proba = self.model.predict_proba(self.X_test)\n",
    "            \n",
    "            if self.y_test is not None:\n",
    "                accuracy = (predictions == self.y_test).mean()\n",
    "                print(f\"   â€¢ Accuracy on test set: {accuracy:.3f}\")\n",
    "            \n",
    "            print(f\"   â€¢ Positive predictions: {predictions.sum()}/{len(predictions)} ({predictions.mean():.1%})\")\n",
    "            print(f\"   â€¢ Average predicted probability: {pred_proba[:, 1].mean():.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   â€¢ Could not calculate performance metrics: {str(e)}\")\n",
    "        \n",
    "        return {\n",
    "            'feature_analysis': feature_analysis,\n",
    "            'risk_factors': risk_factors,\n",
    "            'protective_factors': protective_factors\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the complete explainability analysis\"\"\"\n",
    "    print(\"ğŸ¥ GALLSTONE PREDICTION MODEL - EXPLAINABILITY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load the explainer\n",
    "    explainer = GallstoneExplainability.load_from_files()\n",
    "    \n",
    "    if explainer is None:\n",
    "        print(\"âŒ Failed to load explainer\")\n",
    "        return\n",
    "    \n",
    "    # Run feature importance analysis\n",
    "    importance_results = explainer.create_feature_importance_comparison()\n",
    "    \n",
    "    # Run SHAP analysis\n",
    "    explainer.setup_shap_explainer()\n",
    "    \n",
    "    # Generate plots\n",
    "    explainer.generate_shap_plots()\n",
    "    \n",
    "    # Generate clinical report\n",
    "    clinical_report = explainer.generate_clinical_report()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‰ ANALYSIS COMPLETE!\")\n",
    "    print(\"âœ… Check the generated files:\")\n",
    "    print(\"   â€¢ shap_plots/ folder for SHAP visualizations\")\n",
    "    print(\"   â€¢ feature_importance_comparison.png\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f1959-f6c7-4fee-b804-22bbc01b2989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
